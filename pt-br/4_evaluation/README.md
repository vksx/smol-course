# Evaluation (Avalia√ß√£o)

A avalia√ß√£o √© uma etapa cr√≠tica no desenvolvimento e implanta√ß√£o de modelos de linguagem. Ela nos ajuda a entender qu√£o bem nossos modelos desempenham em diferentes capacidades e a identificar √°reas para melhorias. Este m√≥dulo aborda benchmarks padr√£o e abordagens de avalia√ß√£o espec√≠ficas de dom√≠nio para avaliar de forma abrangente o seu modelo smol (miudinho).

Usaremos o [`lighteval`](https://github.com/huggingface/lighteval), uma poderosa biblioteca de avalia√ß√£o desenvolvida pelo Hugging Face que se integra perfeitamente ao ecossistema Hugging Face. Para um aprendizado mais profundo nos conceitos e pr√°ticas recomendadas de avalia√ß√£o, confira o [guia](https://github.com/huggingface/evaluation-guidebook).

## Vis√£o Geral do M√≥dulo 

Uma estrat√©gia de avalia√ß√£o completa examina m√∫ltiplos aspectos do desempenho do modelo. Avaliamos capacidades espec√≠ficas de tarefas, como responder a perguntas e sumariza√ß√£o, para entender como o modelo lida com diferentes tipos de problemas. Medimos a qualidade do output atrav√©s de fatores como coer√™ncia e precis√£o factual. A avalia√ß√£o de seguran√ßa ajuda a identificar outputs potencialmente prejudiciais ou biases. Por fim, os testes de especializa√ß√£o de dom√≠nio verificam o conhecimento especializado do modelo no campo-alvo.

## Conte√∫do

### 1Ô∏è‚É£ [Benchmarks Autom√°ticos](./automatic_benchmarks.md)

Aprenda a avaliar seu modelo usando benchmarks e m√©tricas padronizados. Exploraremos benchmarks comuns, como MMLU e TruthfulQA, entenderemos as principais m√©tricas e configura√ß√µes de avalia√ß√£o e abordaremos as melhores pr√°ticas para avalia√ß√µes reproduz√≠veis.

### 2Ô∏è‚É£ [Avalia√ß√£o de Domn√≠nio Personalizado](./custom_evaluation.md)

Descubra como criar pipelines de avalia√ß√£o adaptados ao seu caso de uso espec√≠fico. Veremos o design de tarefas de avalia√ß√£o personalizadas, implementa√ß√£o de m√©tricas especializadas e constru√ß√£o de conjuntos de dados de avalia√ß√£o que atendam √†s suas necessidades.

### 3Ô∏è‚É£ [Projeto de Avalia√ß√£o de Dom√≠nio](./project/README.md)

Siga um exemplo completo de constru√ß√£o de um pipeline de avalia√ß√£o de dom√≠nio espec√≠fico. Voc√™ aprender√° a gerar conjuntos de dados de avalia√ß√£o, usar o Argilla para anota√ß√£o de dados, criar conjuntos de dados padronizados e avaliar modelos usando o LightEval.

### Cadernos de Exerc√≠cio

| T√≠tulo | Descri√ß√£o | Exerc√≠cio | Link | Colab |
|-------|-------------|----------|------|-------|
| Avalie e Analise Seu LLM | Aprenda a usar o LightEval para avaliar e comparar modelos em dom√≠nios espec√≠ficos | üê¢ Use tarefas do dom√≠nio da medicina para avaliar um modelo <br> üêï Crie uma nova avalia√ß√£o de dom√≠nio com diferentes tarefas do MMLU <br> ü¶Å Crie uma tarefa de avalia√ß√£o personalizada para o seu dom√≠nio | [Notebook](./notebooks/lighteval_evaluate_and_analyse_your_LLM.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/4_evaluation/notebooks/lighteval_evaluate_and_analyse_your_LLM.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |


## Refer√™ncias

- [Guia de Avalia√ß√£o](https://github.com/huggingface/evaluation-guidebook) - Guia abrangente de avalia√ß√£o de LLMs
- [Documenta√ß√£o do LightEval](https://github.com/huggingface/lighteval) - Documenta√ß√£o oficial do m√≥dulo LightEval
- [Documenta√ß√£o do Argilla](https://docs.argilla.io) - Saiba mais sobre a plataforma de anota√ß√£o Argilla
- [Artigo do MMLU](https://arxiv.org/abs/2009.03300) - Artigo que descreve o benchmark MMLU
- [Criando uma Tarefa Personalizada](https://github.com/huggingface/lighteval/wiki/Adding-a-Custom-Task)
- [Criando uma M√©trica Personalizada](https://github.com/huggingface/lighteval/wiki/Adding-a-New-Metric)
- [Usando M√©tricas Existentes](https://github.com/huggingface/lighteval/wiki/Metric-List)