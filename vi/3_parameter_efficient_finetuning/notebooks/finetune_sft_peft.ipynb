{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-6LLOPZouLg"
   },
   "source": [
    "# C√°ch tinh ch·ªânh LLM v·ªõi LoRA Adapters s·ª≠ d·ª•ng Hugging Face TRL\n",
    "\n",
    "Trong Notebook n√†y b·∫°n s·∫Ω ƒë∆∞·ª£c h·ªçc c√°ch tinh ch·ªânh hi·ªáu qu·∫£ c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn s·ª≠ d·ª•ng LoRA (Low-Rank Adaptation) adapters. LoRA l√† m·ªôt k·ªπ thu·∫≠t tinh ch·ªânh tham s·ªë hi·ªáu qu·∫£:\n",
    "- ƒê√≥ng bƒÉng c√°c tr·ªçng s·ªë m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán tr∆∞·ªõc\n",
    "- Th√™m c√°c ma tr·∫≠n ph√¢n r√£ h·∫°ng nh·ªè c√≥ th·ªÉ hu·∫•n luy·ªán v√†o c√°c l·ªõp attention  \n",
    "- Th∆∞·ªùng gi·∫£m kho·∫£ng 90% tham s·ªë c√≥ th·ªÉ hu·∫•n luy·ªán\n",
    "- Duy tr√¨ hi·ªáu su·∫•t m√¥ h√¨nh trong khi s·ª≠ d·ª•ng b·ªô nh·ªõ hi·ªáu qu·∫£\n",
    "\n",
    "Ch√∫ng ta s·∫Ω t√¨m hi·ªÉu:  \n",
    "1. C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng ph√°t tri·ªÉn v√† c·∫•u h√¨nh LoRA\n",
    "2. T·∫°o v√† chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán adapter \n",
    "3. Tinh ch·ªânh s·ª≠ d·ª•ng `trl` v√† `SFTTrainer` v·ªõi LoRA adapters\n",
    "4. Ki·ªÉm tra m√¥ h√¨nh v√† g·ªôp adapters (t√πy ch·ªçn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXqd9BXgouLi"
   },
   "source": [
    "## 1. C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng ph√°t tri·ªÉn\n",
    "\n",
    "B∆∞·ªõc ƒë·∫ßu ti√™n l√† c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán Hugging Face v√† PyTorch, bao g·ªìm `trl, transformers v√† datasets`. N·∫øu b·∫°n ch∆∞a nghe n√≥i v·ªÅ `trl`, ƒë·ª´ng lo l·∫Øng. ƒê√≥ l√† m·ªôt th∆∞ vi·ªán m·ªõi tr√™n n·ªÅn t·∫£ng transformers v√† datasets, gi√∫p vi·ªác tinh ch·ªânh c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (LLM) tr·ªü n√™n d·ªÖ d√†ng h∆°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKvGVxImouLi"
   },
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c y√™u c·∫ßu trong Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# X√°c th·ª±c v·ªõi Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n",
    "# ƒë·ªÉ thu·∫≠n ti·ªán b·∫°n c√≥ th·ªÉ t·∫°o m·ªôt bi·∫øn m√¥i tr∆∞·ªùng ch·ª©a token hub c·ªßa b·∫°n l√† HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHUzfwpKouLk"
   },
   "source": [
    "## 2. T·∫£i d·ªØ li·ªáu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "z4p6Bvo7ouLk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 2260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T·∫£i m·ªôt t·∫≠p d·ªØ li·ªáu m·∫´u\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: x√°c ƒë·ªãnh dataset v√† config c·ªßa b·∫°n s·ª≠ d·ª•ng c√°c tham s·ªë path v√† name\n",
    "dataset = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TOhJdtsouLk"
   },
   "source": [
    "## 3. Tinh ch·ªânh LLM s·ª≠ d·ª•ng `trl` v√† `SFTTrainer` v·ªõi LoRA\n",
    "\n",
    "[SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) t·ª´ `trl` cung c·∫•p t√≠ch h·ª£p v·ªõi LoRA adapters th√¥ng qua th∆∞ vi·ªán [PEFT](https://huggingface.co/docs/peft/en/index). Nh·ªØng l·ª£i th·∫ø ch√≠nh c·ªßa c√†i ƒë·∫∑t n√†y bao g·ªìm:\n",
    "\n",
    "1. **Hi·ªáu qu·∫£ b·ªô nh·ªõ**:\n",
    "   - Ch·ªâ c√°c tham s·ªë adapter ƒë∆∞·ª£c l∆∞u tr·ªØ trong b·ªô nh·ªõ GPU \n",
    "   - Tr·ªçng s·ªë m√¥ h√¨nh c∆° s·ªü v·∫´n ƒë√≥ng bƒÉng v√† c√≥ th·ªÉ ƒë∆∞·ª£c t·∫£i v·ªõi ƒë·ªô ch√≠nh x√°c th·∫•p h∆°n\n",
    "   - Cho ph√©p tinh ch·ªânh c√°c m√¥ h√¨nh l·ªõn tr√™n GPU ti√™u d√πng\n",
    "\n",
    "2. **T√≠nh nƒÉng hu·∫•n luy·ªán**:\n",
    "   - T√≠ch h·ª£p PEFT/LoRA s·∫µn c√≥ v·ªõi c√†i ƒë·∫∑t t·ªëi thi·ªÉu\n",
    "   - H·ªó tr·ª£ QLoRA (LoRA l∆∞·ª£ng t·ª≠ h√≥a) cho hi·ªáu qu·∫£ b·ªô nh·ªõ t·ªët h∆°n\n",
    "\n",
    "3. **Qu·∫£n l√Ω Adapter**: \n",
    "   - L∆∞u tr·ªçng s·ªë adapter trong qu√° tr√¨nh checkpoint\n",
    "   - T√≠nh nƒÉng g·ªôp adapters tr·ªü l·∫°i m√¥ h√¨nh c∆° s·ªü\n",
    "\n",
    "Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng LoRA trong v√≠ d·ª• c·ªßa m√¨nh, **k·∫øt h·ª£p LoRA v·ªõi l∆∞·ª£ng t·ª≠ h√≥a 4-bit** ƒë·ªÉ gi·∫£m th√™m vi·ªác s·ª≠ d·ª•ng b·ªô nh·ªõ m√† kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn hi·ªáu su·∫•t. C√†i ƒë·∫∑t ch·ªâ y√™u c·∫ßu m·ªôt v√†i b∆∞·ªõc c·∫•u h√¨nh:\n",
    "1. X√°c ƒë·ªãnh c·∫•u h√¨nh LoRA (rank, alpha, dropout)\n",
    "2. T·∫°o SFTTrainer v·ªõi c·∫•u h√¨nh PEFT \n",
    "3. Hu·∫•n luy·ªán v√† l∆∞u tr·ªçng s·ªë adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°c th∆∞ vi·ªán c·∫ßn thi·∫øt \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# T·∫£i m√¥ h√¨nh v√† tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Thi·∫øt l·∫≠p ƒë·ªãnh d·∫°ng chat\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# ƒê·∫∑t t√™n cho b·∫£n tinh ch·ªânh ƒë·ªÉ l∆∞u &/ t·∫£i l√™n\n",
    "finetune_name = \"SmolLM2-FT-LoRA-Adapter\"\n",
    "finetune_tags = [\"smol-course\", \"module_3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbuVArTHouLk"
   },
   "source": [
    "`SFTTrainer` h·ªó tr·ª£ t√≠ch h·ª£p s·∫µn v·ªõi `peft`, ƒëi·ªÅu n√†y gi√∫p tinh ch·ªânh hi·ªáu qu·∫£ c√°c LLM d·ªÖ d√†ng h∆°n b·∫±ng c√°ch s·ª≠ d·ª•ng, v√≠ d·ª• nh∆∞ LoRA. Ch√∫ng ta ch·ªâ c·∫ßn t·∫°o `LoraConfig` v√† cung c·∫•p n√≥ cho trainer.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>B√†i t·∫≠p: X√°c ƒë·ªãnh tham s·ªë LoRA cho tinh ch·ªânh</h2>\n",
    "    <p>L·∫•y m·ªôt b·ªô d·ªØ li·ªáu t·ª´ Hugging Face hub v√† tinh ch·ªânh m·ªôt m√¥ h√¨nh tr√™n n√≥.</p>\n",
    "    <p><b>M·ª©c ƒë·ªô kh√≥</b></p> \n",
    "    <p>üê¢ S·ª≠ d·ª•ng c√°c tham s·ªë chung cho m·ªôt b·∫£n tinh ch·ªânh t√πy √Ω</p>\n",
    "    <p>üêï ƒêi·ªÅu ch·ªânh c√°c tham s·ªë v√† so s√°nh trong weights & biases.</p>\n",
    "    <p>ü¶Å ƒêi·ªÅu ch·ªânh c√°c tham s·ªë v√† cho th·∫•y s·ª± thay ƒë·ªïi trong k·∫øt qu·∫£ suy lu·∫≠n.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blDSs9swouLk"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: ƒêi·ªÅu ch·ªânh c·∫•u h√¨nh tham s·ªë LoRA\n",
    "# r: chi·ªÅu rank cho ma tr·∫≠n c·∫≠p nh·∫≠t LoRA (nh·ªè h∆°n = n√©n nhi·ªÅu h∆°n)\n",
    "rank_dimension = 4\n",
    "# lora_alpha: h·ªá s·ªë t·ª∑ l·ªá cho c√°c l·ªõp LoRA (cao h∆°n = ƒëi·ªÅu ch·ªânh m·∫°nh h∆°n) \n",
    "lora_alpha = 8\n",
    "# lora_dropout: x√°c su·∫•t dropout cho c√°c l·ªõp LoRA (gi√∫p tr√°nh overfitting)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # Chi·ªÅu rank - th∆∞·ªùng t·ª´ 4-32\n",
    "    lora_alpha=lora_alpha,  # H·ªá s·ªë t·ª∑ l·ªá LoRA - th∆∞·ªùng g·∫•p 2 l·∫ßn rank\n",
    "    lora_dropout=lora_dropout,  # X√°c su·∫•t dropout cho c√°c l·ªõp LoRA \n",
    "    bias=\"none\",  # Lo·∫°i bias cho LoRA. c√°c bias t∆∞∆°ng ·ª©ng s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t trong qu√° tr√¨nh hu·∫•n luy·ªán.\n",
    "    target_modules=\"all-linear\",  # Nh·ªØng module n√†o √°p d·ª•ng LoRA\n",
    "    task_type=\"CAUSAL_LM\",  # Lo·∫°i t√°c v·ª• cho ki·∫øn tr√∫c m√¥ h√¨nh\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5NUDPcaouLl"
   },
   "source": [
    "Tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán, ch√∫ng ta c·∫ßn x√°c ƒë·ªãnh c√°c `si√™u tham s·ªë (TrainingArguments)` m√† ch√∫ng ta mu·ªën s·ª≠ d·ª•ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqT28VZlouLl"
   },
   "outputs": [],
   "source": [
    "# C·∫•u h√¨nh hu·∫•n luy·ªán  \n",
    "# Si√™u tham s·ªë d·ª±a tr√™n khuy·∫øn ngh·ªã t·ª´ b√†i b√°o QLoRA \n",
    "args = SFTConfig(\n",
    "    # C√†i ƒë·∫∑t ƒë·∫ßu ra\n",
    "    output_dir=finetune_name,  # Th∆∞ m·ª•c ƒë·ªÉ l∆∞u checkpoint m√¥ h√¨nh\n",
    "    # Th·ªùi gian hu·∫•n luy·ªán\n",
    "    num_train_epochs=1,  # S·ªë epoch hu·∫•n luy·ªán\n",
    "    # C√†i ƒë·∫∑t k√≠ch th∆∞·ªõc batch \n",
    "    per_device_train_batch_size=2,  # K√≠ch th∆∞·ªõc batch cho m·ªói GPU\n",
    "    gradient_accumulation_steps=2,  # T√≠ch l≈©y gradient cho batch hi·ªáu qu·∫£ l·ªõn h∆°n\n",
    "    # T·ªëi ∆∞u b·ªô nh·ªõ\n",
    "    gradient_checkpointing=True,  # ƒê√°nh ƒë·ªïi t√≠nh to√°n ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ\n",
    "    # C√†i ƒë·∫∑t optimizer\n",
    "    optim=\"adamw_torch_fused\",  # S·ª≠ d·ª•ng AdamW fusion cho hi·ªáu qu·∫£\n",
    "    learning_rate=2e-4,  # T·ªëc ƒë·ªô h·ªçc (t·ª´ b√†i b√°o QLoRA)\n",
    "    max_grad_norm=0.3,  # Ng∆∞·ª°ng c·∫Øt gradient\n",
    "    # L·ªãch tr√¨nh h·ªçc\n",
    "    warmup_ratio=0.03,  # Ph·∫ßn b∆∞·ªõc cho warmup\n",
    "    lr_scheduler_type=\"constant\",  # Gi·ªØ t·ªëc ƒë·ªô h·ªçc kh√¥ng ƒë·ªïi sau warmup\n",
    "    # Ghi log v√† l∆∞u\n",
    "    logging_steps=10,  # Ghi metrics m·ªói N b∆∞·ªõc\n",
    "    save_strategy=\"epoch\",  # L∆∞u checkpoint m·ªói epoch\n",
    "    # C√†i ƒë·∫∑t ƒë·ªô ch√≠nh x√°c\n",
    "    bf16=True,  # S·ª≠ d·ª•ng ƒë·ªô ch√≠nh x√°c bfloat16\n",
    "    # C√†i ƒë·∫∑t t√≠ch h·ª£p\n",
    "    push_to_hub=False,  # Kh√¥ng ƒë·∫©y l√™n HuggingFace Hub\n",
    "    report_to=None,  # T·∫Øt ghi log b√™n ngo√†i\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGhR7uFBouLl"
   },
   "source": [
    "B√¢y gi·ªù ch√∫ng ta ƒë√£ c√≥ m·ªçi th·ª© c·∫ßn thi·∫øt ƒë·ªÉ t·∫°o `SFTTrainer` v√† b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh c·ªßa m√¨nh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M00Har2douLl"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 1512  # ƒë·ªô d√†i chu·ªói t·ªëi ƒëa cho m√¥ h√¨nh v√† ƒë√≥ng g√≥i (packing) b·ªô d·ªØ li·ªáu\n",
    "\n",
    "# T·∫°o SFTTrainer v·ªõi c·∫•u h√¨nh LoRA\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,  # C·∫•u h√¨nh LoRA\n",
    "    max_seq_length=max_seq_length,  # ƒê·ªô d√†i chu·ªói t·ªëi ƒëa \n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,  # B·∫≠t ƒë√≥ng g√≥i ƒë·∫ßu v√†o cho hi·ªáu qu·∫£ \n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # Token ƒë·∫∑c bi·ªát ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi template\n",
    "        \"append_concat_token\": False,  # Kh√¥ng c·∫ßn th√™m separator \n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ_kRN24ouLl"
   },
   "source": [
    "B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh b·∫±ng c√°ch g·ªçi ph∆∞∆°ng th·ª©c `train()` tr√™n `Trainer` c·ªßa ch√∫ng ta. Vi·ªác n√†y s·∫Ω b·∫Øt ƒë·∫ßu v√≤ng l·∫∑p hu·∫•n luy·ªán v√† hu·∫•n luy·ªán m√¥ h√¨nh c·ªßa ch√∫ng ta trong `3 epochs`. V√¨ ch√∫ng ta ƒëang s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p PEFT, ch√∫ng ta s·∫Ω ch·ªâ l∆∞u ph·∫ßn tr·ªçng s·ªë c·ªßa adapter ƒë√£ ƒëi·ªÅu ch·ªânh v√† kh√¥ng l∆∞u to√†n b·ªô m√¥ h√¨nh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq4nIYqKouLl"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300e5dfbb4b54750b77324345c7591f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=72, training_loss=1.6402628521124523, metrics={'train_runtime': 195.2398, 'train_samples_per_second': 1.485, 'train_steps_per_second': 0.369, 'total_flos': 282267289092096.0, 'train_loss': 1.6402628521124523, 'epoch': 0.993103448275862})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán, m√¥ h√¨nh s·∫Ω t·ª± ƒë·ªông ƒë∆∞·ª£c l∆∞u l√™n hub v√† th∆∞ m·ª•c ƒë·∫ßu ra\n",
    "trainer.train()\n",
    "\n",
    "# l∆∞u m√¥ h√¨nh\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4HHSYYzouLl"
   },
   "source": [
    "Vi·ªác hu·∫•n luy·ªán v·ªõi Flash Attention cho 3 epoch v·ªõi m·ªôt dataset 15k m·∫´u m·∫•t `4:14:36` tr√™n m·ªôt c·ª•m m√°y `g5.2xlarge` c·ªßa AWS. C·ª•m m√°y n√†y c√≥ gi√° `1.21$/h` v√† t·ªïng chi ph√≠ c·ªßa l·∫ßn hu·∫•n luy·ªán n√†y ch·ªâ t·ªën kho·∫£ng `5.3$`.\n",
    "\n",
    "**Ghi ch√∫: b·∫°n ho√†n to√†n c√≥ th·ªÉ s·ª≠ d·ª•ng GPU c·ªßa Kaggle ho·∫∑c Google Colab ƒë·ªÉ hu·∫•n luy·ªán**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C309KsXjouLl"
   },
   "source": [
    "### G·ªôp LoRA Adapter v√†o M√¥ h√¨nh G·ªëc\n",
    "\n",
    "Khi s·ª≠ d·ª•ng LoRA, ch√∫ng ta ch·ªâ hu·∫•n luy·ªán tr·ªçng s·ªë adapter trong khi gi·ªØ nguy√™n m√¥ h√¨nh c∆° s·ªü. Trong qu√° tr√¨nh hu·∫•n luy·ªán, ch√∫ng ta ch·ªâ l∆∞u nh·ªØng tr·ªçng s·ªë adapter nh·∫π n√†y (~2-10MB) thay v√¨ m·ªôt b·∫£n sao m√¥ h√¨nh ƒë·∫ßy ƒë·ªß. Tuy nhi√™n, ƒë·ªÉ tri·ªÉn khai, b·∫°n c√≥ th·ªÉ mu·ªën g·ªôp c√°c adapter tr·ªü l·∫°i m√¥ h√¨nh c∆° s·ªü ƒë·ªÉ:\n",
    "\n",
    "1. **ƒê∆°n gi·∫£n h√≥a tri·ªÉn khai**: M·ªôt file m√¥ h√¨nh thay v√¨ m√¥ h√¨nh c∆° s·ªü + adapters\n",
    "2. **T·ªëc ƒë·ªô suy lu·∫≠n**: Kh√¥ng c√≥ chi ph√≠ t√≠nh to√°n adapter ph·ª• th√™m\n",
    "3. **T∆∞∆°ng th√≠ch Framework**: T∆∞∆°ng th√≠ch t·ªët h∆°n v·ªõi c√°c framework ph·ª•c v·ª•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "# T·∫£i m√¥ h√¨nh PEFT tr√™n CPU  \n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# G·ªôp LoRA v√† m√¥ h√¨nh c∆° s·ªü v√† l∆∞u\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\n",
    "    args.output_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yO6E9quouLl"
   },
   "source": [
    "## 3. Ki·ªÉm tra M√¥ h√¨nh\n",
    "\n",
    "Sau khi hu·∫•n luy·ªán ho√†n t·∫•t, ch√∫ng ta mu·ªën ki·ªÉm tra m√¥ h√¨nh c·ªßa m√¨nh. Ch√∫ng ta s·∫Ω t·∫£i c√°c m·∫´u kh√°c nhau t·ª´ dataset g·ªëc v√† ƒë√°nh gi√° m√¥ h√¨nh tr√™n nh·ªØng m·∫´u ƒë√≥, s·ª≠ d·ª•ng m·ªôt v√≤ng l·∫∑p ƒë∆°n gi·∫£n v√† ƒë·ªô ch√≠nh x√°c l√†m ƒëi·ªÉm s·ªë ƒë√°nh gi√°.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>B√†i t·∫≠p Bonus: T·∫£i LoRA Adapter</h2>\n",
    "    <p>S·ª≠ d·ª•ng nh·ªØng g√¨ b·∫°n ƒë√£ h·ªçc ƒë∆∞·ª£c t·ª´ notebook v√≠ d·ª• ƒë·ªÉ t·∫£i adapter LoRA ƒë√£ hu·∫•n luy·ªán c·ªßa b·∫°n cho suy lu·∫≠n.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "I5B494OdouLl"
   },
   "outputs": [],
   "source": [
    "# gi·∫£i ph√≥ng b·ªô nh·ªõ m·ªôt l·∫ßn n·ªØa\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1UhohVdouLl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM \n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# T·∫£i M√¥ h√¨nh v·ªõi PEFT adapter\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetune_name)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    finetune_name, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=merged_model, tokenizer=tokenizer, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99uFDAuuouLl"
   },
   "source": [
    "H√£y th·ª≠ m·ªôt s·ªë ch·ªâ ƒë·ªãnh m·∫´u v√† xem m√¥ h√¨nh ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-shSmUbvouLl",
    "outputId": "16d97c61-3b31-4040-c780-3c4de75c3824"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Th·ªß ƒë√¥ c·ªßa Vi·ªát Nam th√†nh ph·ªë n√†o? Gi·∫£i th√≠ch t·∫°i sao l√† nh∆∞ v·∫≠y v√† li·ªáu n√≥ c√≥ kh√°c trong qu√° kh·ª© kh√¥ng?\",\n",
    "    \"Vi·∫øt m·ªôt h√†m Python ƒë·ªÉ t√≠nh giai th·ª´a c·ªßa m·ªôt s·ªë.\",\n",
    "    \"M·ªôt khu v∆∞·ªùn h√¨nh ch·ªØ nh·∫≠t c√≥ chi·ªÅu d√†i 25 m√©t v√† chi·ªÅu r·ªông 15 m√©t. N·∫øu b·∫°n mu·ªën x√¢y m·ªôt h√†ng r√†o xung quanh to√†n b·ªô khu v∆∞·ªùn, b·∫°n s·∫Ω c·∫ßn bao nhi√™u m√©t h√†ng r√†o?\",\n",
    "    \"S·ª± kh√°c bi·ªát gi·ªØa tr√°i c√¢y v√† rau c·ªß l√† g√¨? ƒê∆∞a ra v√≠ d·ª• cho m·ªói lo·∫°i.\",\n",
    "]\n",
    "\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt):].strip()\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
